We care about [open science](https://en.wikipedia.org/wiki/Open_science). Because our research is almost entirely publicly funded, and because all our permanent and non-permanent positions are civil servants [part of the public sector](https://en.wikipedia.org/wiki/French_National_Centre_for_Scientific_Research), we believe it is a moral commitment to make our results accessible for free at all levels of society. This includes publications (open-access) but also research material, stimuli, data, and software. 

Click on the ![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"} banners below for direct access to the PDFs of the papers; <br> 
on ![code](images/site_icons/code-sansbg.png){: style="height:20px;width:100px"} for a link to the project's code hosted on the group's [github page](https://github.com/neuro-team-femto); <br>
and on ![data](images/site_icons/data-sansbg.png){: style="height:20px;width:100px"} for a link to the associated research data (stimuli and results). 

Our overt objective with the following is that 100% of our publications are associated with all three banners. If one is missing for a specific project, there's a good chance that we're working on it - do feel free to request as you need. 

Since 2021, all our research data is doi-indexed in the [Université de Bourgogne Franche-Comté](https://www.ubfc.fr/)'s Open Data portal [Dat@UBFC](https://dataosu.obs-besancon.fr). In addition to resources specific to papers below, we also develop and maintain a number of open-source research software, including reverse-correlation toolbox [CLEESE](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205943) and vocal-feedback plateform [DAVID](https://link.springer.com/article/10.3758/s13428-017-0873-y), which can be found on our [Resources page](resources.md).  


<hr>

### 2023

[_Separating representational and noise components of speech prosody perception after brain stroke_](https://www.medrxiv.org/content/10.1101/2023.10.17.23297140v2) <br>
*Adl Zarrabi, A.*, Jeulin, M., Bardet, P., Commere, P., Naccache, L., *Aucouturier, J. J.*, Ponsot, E. & *Villain, M.* <br>
MedRxiv, 2023-10, 2023. <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2023/adl_zarrabi_medRxiv_2023.pdf)


[_The psychophysics of empathy: Using reverse-correlation to quantify the overlap between self & other representations of emotional expressions_](https://osf.io/preprints/psyarxiv/rdmve) <br>
Zaied, S, Soladié, C. & *Aucouturier, J.J.* <br>
PsyArXiv rdmve, 2023. <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2023/zaied_psyarxiv_2023.pdf)
[![data](images/site_icons/data-sansbg.png){: style="height:20px;width:100px"}](https://github.com/neuro-team-femto/empathy)
[![code](images/site_icons/code-sansbg.png){: style="height:20px;width:100px"}](https://github.com/neuro-team-femto/empathy)


[_Cracking the pitch code of music-motor synchronization using data-driven methods_](https://osf.io/preprints/psyarxiv/zkbn3) <br>
Migotti, L., *Decultot, Q.*, Grailhe, P. & *Aucouturier, J. J.* <br>
PsyArXiv zkbn3, 2023. <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2023/migotti_psyarxiv_2023.pdf)
[![data](images/site_icons/data-sansbg.png){: style="height:20px;width:100px"}](https://github.com/neuro-team-femto/treadmill)
[![code](images/site_icons/code-sansbg.png){: style="height:20px;width:100px"}](https://github.com/neuro-team-femto/treadmill)


[_Pupil dilation reflects the dynamic integration of audiovisual emotional speech_](https://www.nature.com/articles/s41598-023-32133-2) <br>
Arias Sarah, P., Hall, L., Saitovitch, A., *Aucouturier, J. J.*, Zilbovicius, M., & Johansson, P. <br>
Scientific reports, 13(1), 5507, 2023. <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2023/arias_scireport_2023.pdf)


[_Algorithmic voice transformations reveal the phonological basis of language-familiarity effects in cross-cultural emotion judgments_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285028) <br>
Nakai, T., Rachman, L., Arias Sarah, P., Okanoya, K., & *Aucouturier, J.J.* <br>
Plos one, 18(5), e0285028, 2023. <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2023/nakai_plos_one_2023.pdf)


[_Combining GAN with reverse correlation to construct personalized facial expressions_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290612) <br>
Yan, S., Soladié, C., *Aucouturier, J. J.*, & Seguier, R. <br>
Plos one, 18(8), e0290612, 2023. <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2023/sen_plos_one_2023.pdf)



### 2022

[_Three simple steps to improve the interpretability of EEG-SVM studies_](https://www.biorxiv.org/content/10.1101/2021.12.14.472588v1)<br>
*Coralie Joucla*, Damien Gabriel, Juan-Pablo Ortega & Emmanuel Haffen<br>
Journal of Neurophysiology, 2022 <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2022/Joucla_Biorxiv_2022.pdf)

[_It’s not what you say, it’s how you say it: a retrospective study of the impact of prosody on own-name P300 in comatose patients_](https://www.sciencedirect.com/science/article/pii/S1388245722000128?via%3Dihub)<br>
*Estelle Pruvost-Robieux*, Nathalie André-Obadia, Angela Marchi, Tarek Sharshar, Marco Liuni, Martine Gavaret & *Jean-Julien Aucouturier*<br>
Clinical Neurophysiology, vol. 135, 2022 <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2022/Pruvost-Robieux_Clinical_Neurophysiology_2022.pdf)

<hr>

### 2021

[_The shallow of your smile: the ethics of expressive vocal deep-fakes_](https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0396) <br>
*Nadia Guerouaou*, Guillaume Vaiva & *JJ Aucouturier*<br>
Philosophical Transations of the Royal Society B, vol. 377 (1841), 2021 <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2021/Guerouaou_Philosophical_Transactions_2021.pdf)
[![data](images/site_icons/data-sansbg.png){: style="height:20px;width:100px"}](https://github.com/creamlab/deep-ethics)
[![code](images/site_icons/code-sansbg.png){: style="height:20px;width:100px"}](https://github.com/creamlab/deep-ethics)

[_Even violins can cry: specifically vocal emotional behaviours also drive the perception of emotions in non-vocal music_](https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0396)<br>
*Daniel Bedoya*, *Pablo Arias*, *Laura Rachman*, Marco Liuni, Clément Canonne, *Louise Goupil* & *JJ Aucouturier*<br>
Philosophical Transations of the Royal Society B, vol. 376(1840), 2021 <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2021/Bedoya_Philosophical_Transactions_2021.pdf)
[![data](images/site_icons/data-sansbg.png){: style="height:20px;width:100px"}](http://dx.doi.org/doi:10.25666/DATAOSU-2022-02-28)
[![code](images/site_icons/code-sansbg.png){: style="height:20px;width:100px"}](https://github.com/creamlab/smiling_violins)

[_Facial mimicry in the congenitally blind_](https://www.cell.com/current-biology/fulltext/S0960-9822(21)01195-7)<br>
*Pablo Arias*, Caren Bellmann & *JJ Aucouturier*<br>
Current Biology, vol. 31(19), PR1112-R1114 (2021) <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2021/Arias_Current_Biology_2021.pdf)

[_Distinct signatures of subjective confidence and objective accuracy in speech prosody_](https://www.sciencedirect.com/science/article/abs/pii/S0010027721000809)<br>
*Louise Goupil* & *JJ Aucouturier*<br>
Cognition, vol. 212, 104661 (2021) <br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2021/Goupil_Cognition_2021.pdf)

[_Vocal signals only impact speakers’ own emotions when they are self-attributed_](https://www.sciencedirect.com/science/article/abs/pii/S1053810020305390?dgcid=coauthor)<br>
*Louise Goupil*, Petter Johansson, Lars Hall & *JJ Aucouturier* <br>
Consciousness & Cognition, vol. 88, 103072 (2021)<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2021/Goupil_Consciousness_Cognition_2021.pdf)

[_Listeners perception of certainty and honesty of another speaker is associated with a common prosodic signature_](https://www.nature.com/articles/s41467-020-20649-4)<br>
*Louise Goupil*, *Emmanuel Ponsot*, Daniel Richardson, Gabriel Reyes & *JJ Aucouturier*<br>
Nature Communications, vol. 12, 861 (2021)<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2021/Goupil_Nature_Communications_2021.pdf)


<hr>

### 2016 - 2020

*Note:*  Articles published before 2020 correspond to work conducted in the [CREAM music neuroscience team](https://cream.ircam.fr) in IRCAM. We list here a selection of publications that are important to our current research. For a complete list of publications on music cognition and vocal emotions from the CREAM team (2016-2020), see the [CREAM archive page](cream.md). For even earlier work on machine learning and audio signal processing, see JJA's [Google Scholar page](https://scholar.google.com/citations?user=jnST06UAAAAJ). 

[_CLEESE: An open-source audio-transformation toolbox for data-driven experiments in speech and music cognition_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205943)<br>
*Juan Jose Burred*, *Emmanuel Ponsot*, *Louise Goupil*, *Marco Liuni* & *JJ Aucouturier*<br>
PLoS one, 14(4), e0205943, 2019<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2019/Burred_PLOS_One_2019.pdf)

[_Cracking the social code of speech prosody using reverse correlation_](https://www.pnas.org/content/115/15/3972)<br>
*Emmanuel Ponsot*, *Juan Jose Burred*, Pascal Belin & *JJ Aucouturier*<br>
Proceedings of the National Academy of Sciences, vol 115 (15) 3972-3977, 2018<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2018/Ponsot_PNAS_2018.pdf)

[_Uncovering mental representations of smiled speech using reverse correlation_](https://asa.scitation.org/doi/10.1121/1.5020989)<br>
*Emmanuel Ponsot*, *Pablo Arias* & *JJ Aucouturier*<br>
Journal of the Acoustical Society of America, vol 143 (1), 2018.<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2018/Ponsot_JASA_2018.pdf)

[_DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech_](https://link.springer.com/article/10.3758/s13428-017-0873-y)<br>
*Laura Rachman*, *Marco Liuni*, *Pablo Arias*, Andreas Lind, Petter Johansson, Lars Hall, Daniel Richardson, Katsumi Watanabe, Stéphanie Dubal & *JJ Aucouturier*<br>
Behaviour Research Methods, vol. 50(1), 323–343, 2017<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2017/Rachman_BRM_2017.pdf)

[_Covert Digital Manipulation of Vocal Emotion Alter Speakers’ Emotional State in a Congruent Direction_](https://www.pnas.org/content/113/4/948)<br>
*JJ Aucouturier*, Petter Johansson, Lars Hall, Rodrigo Segnini, Lolita Mercadié & Katsumi Watanabe<br>
Proceedings of the National Academy of Sciences, vol. 113 no. 4, 2016<br>
[![access](images/site_icons/access-sansbg.png){: style="height:20px;width:100px"}](articles/2016/Aucouturier_PNAS_2016.pdf)


<hr>

### Copyright Notice

The documents listed here are available for downloading and have been provided as a means to ensure timely dissemination of scholarly and technical work on a noncommercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may not be re-posted without the explicit permission of the copyright holder.
